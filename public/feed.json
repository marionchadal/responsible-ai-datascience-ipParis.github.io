{
  "version": "https://jsonfeed.org/version/1",
  "title": "Bloggin on Responsible AI",
  "home_page_url": "http://localhost:1313/",
  "feed_url": "http://localhost:1313/feed.json",
  "description": "Bloggin on Responsible AI",
  "favicon": "http://localhost:1313//assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Students from M2 Data Science IP Paris",
    "url": "http://localhost:1313/"
  },
  "items": [
    
    

    
    {
      "id": "d7ac2f8435ec113c47081837ac2b76e60dcb8690",
      "title": "Packed Ensembles",
      "summary": "",
      "content_text": "\rIntroduction\rThe document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets, discussions on the implications of PE's design choices, and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.\rPresentation of the model\rPacked-Ensembles\nThe base network and Packed-Ensembles\nPacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\nBenefits of Packed-Ensembles\nPacked-Ensembles offer several benefits over traditional ensemble methods, including:\nEfficiency: Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\nAccuracy: Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\nCalibration: Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\nOut-of-distribution (OOD) detection: Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\nComparison to other ensemble methods\nThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\nPacked-Ensembles: A Technique for Efficient Neural Network Ensembles\rPacked-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\nUnderstanding Convolutional Layers and Grouped Convolutions:\nConvolutional Layers: These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows: $z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\nwhere:\n$c$ represents the channel index\n$h^j$ denotes the input feature map\n$ω^j$ represents the weight tensor (kernel)\n$⋆$ denotes the 2D cross-correlation operator\nGrouped Convolutions: This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating independent subnetworks. The mathematical formulation for grouped convolutions is given by:\n$$ z^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right) $$\nwhere:\n$γ$ represents the number of groups $C_{j+1}$ and $C_j$ denote the number of output and input channels, respectively. The formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask $\\text{mask}_{m}^j$ $\\in { 0, 1 }^{C_{j+1} \\times C_j \\times s_j^2}$ with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\nThe condition $\\text{mask}_{m}^j(k, l, :, :) = 1$ happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\nComplete Mask and Convolution: $\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$. $z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here: $z^{j+1}$: Output feature map of the layer. $h^j$: Input feature map. $ω^j$: Convolution weights for layer j. $\\otimes$: Denotes convolution operation. $◦$: Denotes Hadamard product (element-wise multiplication). In simpler terms:\nGrouped convolution divides the input channels and weights into groups. A separate mask is created for each group, ensuring elements within a group are aligned. These masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group. The final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask. Background on Deep Ensembles\rThis section delves into Deep Ensembles (DE), a technique for image classification tasks.\nDeep Ensembles\nSetting the Scene\nWe have a dataset $D$ containing pairs of images and their corresponding labels:\n$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width). $y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes). The dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\nA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\n$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$. Traditional Approach:\nThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\nIntroducing Deep Ensembles\nDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\nThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\n$$ P(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m) $$\nThis essentially combines the outputs of multiple networks to create a more robust prediction.\nIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\nBuilding Packed-Ensembles:\nPacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\nSubnetworks: The ensemble is formed by creating $M$ smaller subnetworks within the main network architecture. These subnetworks share the same structure but have independent parameters due to the use of grouped convolutions. Hyperparameters: Packed-Ensembles are defined by three hyperparameters: $α$ (alpha): expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters). $M$: number of subnetworks in the ensemble (represents the ensemble size). $γ$ (gamma): number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity). Mathematical Implementation:\nThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\n$$ \\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j $$\nwhere:\n$\\hat{y}$ represents the ensemble\u0026rsquo;s predicted label $P(y|θ_a^m, x)$ denotes the probability of class $y$ given the input $x$ and the parameters $θ_a^m$ of the $m-th$ subnetwork $\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$ represents the parameters of the $m-th$ subnetwork, obtained by applying element-wise multiplication ($∘$) between the expanded weights ($\\omega_j^{\\alpha}$) and the group mask ($\\text{mask}_{m}$) for each layer $j$ Implementation\nEquivalent architectures for Packed-Ensembles\nThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\nDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\nExperiments\rThe experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\nDatasets: CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels. Architectures: PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. Metrics: Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used. Implementation Details: Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift. Code: PyTorch-Lightning framework is used for implementation. Results\rThe experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\nCIFAR-10/100: PE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet). Smaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100. ImageNet: PE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models. PE achieves better accuracy with a reasonable increase in training and inference cost. These results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\nPacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\nEthics\rThis section emphasizes the ethical considerations of the research. Here are the key points:\nGoal: This research proposes a method to improve uncertainty estimation in deep learning models. Limitations: The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications. Concerns: The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like: Unknown situations Corner cases (uncommon but important situations) Adversarial attacks (attempts to intentionally mislead the model) Potential biases in the model Overall: The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems. Reproducibility: Packed-Ensemble on CIFAR-10\rWe attempted to reproduce the experiment outlined in the tutorial available at https://github.com/ENSTA-U2IS-AI/torch-uncertainty which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\nData Loading and Preprocessing: Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images. Packed-Ensemble Definition: Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture. Loss Function and Optimizer: Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training. Training: Training the Packed-Ensemble model on the CIFAR-10 training data. Testing and Evaluation: Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance, as suggested by the tutorial. Experimental Runs and Observations:\nTest 1:\nGroundTruth: cat ship ship plane\nThe predicted labels are: cat ship ship ship\nTest 2:\nGroundTruth: dog bird horse bird\nThe predicted labels are: dog frog car dog\nTest 3:\nGroundTruth: dog truck plane car The predicted labels are: dog horse ship truck\nChallenges and Limitations:\nA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\n",
      "content_html": "\u003cscript\r\ntype=\"text/x-mathjax-config\"\u003e\r\n\r\nMathJax.Hub.Config({\r\n\r\n    tex2jax: {\r\n\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n\r\n    }\r\n\r\n});\r\n\r\nMathJax.Hub.Queue(function() {\r\n\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n\r\n    }\r\n\r\n});\r\n\r\n\u003c/script\u003e\r\n\u003cscript\r\ntype=\"text/javascript\"\r\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eIntroduction\u003c/h1\u003e\r\n\u003c/div\u003e\r\nThe document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets, discussions on the implications of PE's design choices, and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.\r\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003ePresentation of the model\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/fig1.jpg\" alt=\"The base network and Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eThe base network and Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003ePacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits of Packed-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer several benefits over traditional ensemble methods, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEfficiency:\u003c/strong\u003e Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalibration:\u003c/strong\u003e Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOut-of-distribution (OOD) detection:\u003c/strong\u003e Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eComparison to other ensemble methods\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003ePacked-Ensembles: A Technique for Efficient Neural Network Ensembles\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003ePacked-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstanding Convolutional Layers and Grouped Convolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Layers:\u003c/strong\u003e These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$c$\u003c/strong\u003e represents the channel index\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$h^j$\u003c/strong\u003e denotes the input feature map\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$ω^j$\u003c/strong\u003e represents the weight tensor (kernel)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$⋆$\u003c/strong\u003e denotes the 2D cross-correlation operator\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGrouped Convolutions:\u003c/strong\u003e This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating \u003cstrong\u003eindependent subnetworks\u003c/strong\u003e. The mathematical formulation for grouped convolutions is given by:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right)\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$\u003c/strong\u003e represents the number of groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$C_{j+1}$\u003c/strong\u003e and \u003cstrong\u003e$C_j$\u003c/strong\u003e denote the number of output and input channels, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask \u003cstrong\u003e$\\text{mask}_{m}^j$\u003c/strong\u003e\n\u003cstrong\u003e$\\in { 0, 1 }^{C_{j+1} \\times C_j \\times s_j^2}$\u003c/strong\u003e with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\u003c/p\u003e\n\u003cp\u003eThe condition \u003cstrong\u003e$\\text{mask}_{m}^j(k, l, :, :) = 1$\u003c/strong\u003e happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComplete Mask and Convolution:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e$\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$.\u003c/li\u003e\n\u003cli\u003e$z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here:\n\u003cul\u003e\n\u003cli\u003e$z^{j+1}$: Output feature map of the layer.\u003c/li\u003e\n\u003cli\u003e$h^j$: Input feature map.\u003c/li\u003e\n\u003cli\u003e$ω^j$: Convolution weights for layer \u003ccode\u003ej\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e$\\otimes$: Denotes convolution operation.\u003c/li\u003e\n\u003cli\u003e$◦$: Denotes Hadamard product (element-wise multiplication).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIn simpler terms:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGrouped convolution divides the input channels and weights into groups.\u003c/li\u003e\n\u003cli\u003eA separate mask is created for each group, ensuring elements within a group are aligned.\u003c/li\u003e\n\u003cli\u003eThese masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group.\u003c/li\u003e\n\u003cli\u003eThe final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eBackground on Deep Ensembles\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eThis section delves into Deep Ensembles (DE), a technique for image classification tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/fig2.png\" alt=\"Deep Ensembles\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDeep Ensembles\u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eSetting the Scene\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe have a dataset $D$ containing pairs of images and their corresponding labels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width).\u003c/li\u003e\n\u003cli\u003e$y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\u003c/p\u003e\n\u003cp\u003eA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraditional Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Deep Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\u003c/p\u003e\n\u003cp\u003eThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\u003c/p\u003e\n\u003cp\u003e$$\nP(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m)\n$$\u003c/p\u003e\n\u003cp\u003eThis essentially combines the outputs of multiple networks to create a more robust prediction.\u003c/p\u003e\n\u003cp\u003eIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBuilding Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSubnetworks:\u003c/strong\u003e The ensemble is formed by creating \u003cstrong\u003e$M$\u003c/strong\u003e smaller subnetworks within the main network architecture. These subnetworks share the same structure but have \u003cstrong\u003eindependent parameters\u003c/strong\u003e due to the use of grouped convolutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameters:\u003c/strong\u003e Packed-Ensembles are defined by three hyperparameters:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$α$ (alpha):\u003c/strong\u003e expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$M$:\u003c/strong\u003e number of subnetworks in the ensemble (represents the ensemble size).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$ (gamma):\u003c/strong\u003e number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical Implementation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$\\hat{y}$\u003c/strong\u003e represents the ensemble\u0026rsquo;s predicted label\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$P(y|θ_a^m, x)$\u003c/strong\u003e denotes the probability of class \u003cstrong\u003e$y$\u003c/strong\u003e given the input \u003cstrong\u003e$x$\u003c/strong\u003e and the parameters \u003cstrong\u003e$θ_a^m$\u003c/strong\u003e of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$\u003c/strong\u003e represents the parameters of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork, obtained by applying element-wise multiplication (\u003cstrong\u003e$∘$\u003c/strong\u003e) between the expanded weights (\u003cstrong\u003e$\\omega_j^{\\alpha}$\u003c/strong\u003e) and the group mask (\u003cstrong\u003e$\\text{mask}_{m}$\u003c/strong\u003e) for each layer \u003cstrong\u003e$j$\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/fig4.png\" alt=\"Equivalent architectures for Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eEquivalent architectures for Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003eThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/fig5.png\" alt=\"subnetwork mask\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\u003c/i\u003e\u003c/p\u003e\r\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eExperiments\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eThe experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDatasets:\u003c/strong\u003e CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitectures:\u003c/strong\u003e PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics:\u003c/strong\u003e Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation Details:\u003c/strong\u003e Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode:\u003c/strong\u003e PyTorch-Lightning framework is used for implementation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eResults\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eThe experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCIFAR-10/100:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet).\u003c/li\u003e\n\u003cli\u003eSmaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImageNet:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models.\u003c/li\u003e\n\u003cli\u003ePE achieves better accuracy with a reasonable increase in training and inference cost.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/fig3.png\" alt=\"ResNet50 performance\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003ePacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\u003c/i\u003e\u003c/p\u003e\r\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eEthics\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eThis section emphasizes the ethical considerations of the research. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e This research proposes a method to improve uncertainty estimation in deep learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcerns:\u003c/strong\u003e The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like:\n\u003cul\u003e\n\u003cli\u003eUnknown situations\u003c/li\u003e\n\u003cli\u003eCorner cases (uncommon but important situations)\u003c/li\u003e\n\u003cli\u003eAdversarial attacks (attempts to intentionally mislead the model)\u003c/li\u003e\n\u003cli\u003ePotential biases in the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOverall:\u003c/strong\u003e The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n\u003ch1\u003eReproducibility: Packed-Ensemble on CIFAR-10\u003c/h1\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eWe attempted to reproduce the experiment outlined in the tutorial available at \u003ca href=\"https://github.com/ENSTA-U2IS-AI/torch-uncertainty\"\u003ehttps://github.com/ENSTA-U2IS-AI/torch-uncertainty\u003c/a\u003e which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Loading and Preprocessing:\u003c/strong\u003e Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePacked-Ensemble Definition:\u003c/strong\u003e Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLoss Function and Optimizer:\u003c/strong\u003e Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining:\u003c/strong\u003e Training the Packed-Ensemble model on the CIFAR-10 training data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTesting and Evaluation:\u003c/strong\u003e Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance, as suggested by the tutorial.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental Runs and Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTest 1:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/Result1.png\" alt=\"First result\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  cat   ship  ship  plane\u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003eThe predicted labels are: cat   ship  ship  ship\u003c/p\u003e\n\u003cp\u003eTest 2:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/Result2.png\" alt=\"Second result\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth: dog bird horse bird\u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003eThe predicted labels are: dog  frog  car  dog\u003c/p\u003e\n\u003cp\u003eTest 3:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\r\n    \u003cimg src=\"/images/Result3.png\" alt=\"Third result\" style=\"display:block; margin:auto;\"\u003e\r\n\u003c/div\u003e\r\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  dog truck plane car \u003c/i\u003e\u003c/p\u003e\r\n\u003cp\u003eThe predicted labels are: dog  horse ship  truck\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges and Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/packed-ensembles/",
      "date_published": "27026-27-09T25:2727:00+01:00",
      "date_modified": "27026-27-09T25:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "aba6ac7516897c4ee7afcdf83296daebaa43622b",
      "title": "Another article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nDo not forget to add the script posted on moodle to enable latex in your blogpost! What a beauty! $y=\\theta_0 + \\theta_1x_1$\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cstyle\r\nTYPE=\"text/css\"\u003e\r\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\r\ntype=\"text/x-mathjax-config\"\u003e\r\n\r\nMathJax.Hub.Config({\r\n\r\n    tex2jax: {\r\n\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n\r\n    }\r\n\r\n});\r\n\r\nMathJax.Hub.Queue(function() {\r\n\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n\r\n    }\r\n\r\n});\r\n\r\n\u003c/script\u003e\r\n\u003cscript\r\ntype=\"text/javascript\"\r\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003cp\u003eDo not forget to add the script posted on moodle to enable latex in your blogpost!\nWhat a beauty! $y=\\theta_0 + \\theta_1x_1$\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-first-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "94409f4b19cf119747df5e05d9507c6fc3ae2286",
      "title": "Title of the article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nStart writing here !\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cp\u003eStart writing here !\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-second-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    }
    
  ]
}